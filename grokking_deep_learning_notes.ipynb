{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries used by the book\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images-na.ssl-images-amazon.com/images/I/61gAY7APCQL._SX397_BO1,204,203,200_.jpg) \n",
    "\n",
    "This jupyter notebook contains my notes while reading [Grokking Deep Learning by Andrew Trask](https://www.manning.com/books/grokking-deep-learning). The book is sold as \"a very gentle introduction to Deep Learning\" and covers the intuition more than the theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introducing Deep Learning\n",
    "\n",
    "Deep Learning (DL) is is an intersection of Machine Learning (ML) & Artificial Intelligence (AI). This book covers the science under the hood of the major DL frameworks so you can understand whats going on when you use popular DL frameworks like Torch, Tensorflow, Keras, etc.\n",
    "\n",
    "The book covers everything past high school maths needed to grok DL. Find a personal problem I'm interested in which to apply DL to. This couldbe anything where there is a dataset to predict another. Trask (the author) used Twiter to predict the stock market, which led him from barely knowing programming to a job at a hedge fund in 18 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Fundamental Concepts\n",
    "\n",
    "DL uses a subset of ML methods, primarily Artificial Neural Networks. \n",
    "\n",
    "## Supervised vs Unsupervised ml\n",
    "\n",
    "Two main types of ML:\n",
    "\n",
    "- Direct imitation, or formally **supervised ml** is basically a computer looking at a dataset A which predicts B, say weather sensor data predicting probability of rain, and trying to figure out the pattern b/w an input (sensor data) and a output set (actual weather), so when given a new input set it can apply the earlier learned pattern and come up with a prediction.\n",
    "- Indirect imitation, or fomally **unsupervised ml** looks at a not previously understoond dataset A and tries to find patterns in it. For example, it sorts data into a bunch of clusters. Clustering is the essensce of unsupervised ml. The computer doesn't know what the clusters mean but thats where the human comes in.\n",
    "\n",
    "## Parametric vs Non-Parametric Learning\n",
    "\n",
    "A parametric model has a fixed number of parameters to change, while a non-parametric model has infinite parameters.\n",
    "\n",
    "Supervised parametric dl models take in input data, process them based on a fixed number of adjustable parameters and makes a prediction. The ml model learns the optimium parameters by comparing its predictions to the actual truth, then going back and tinkering with the parameters.\n",
    "\n",
    "Unsupervised parametric dl models are similar to the supervised since they also use parameters, but they cluster the data into groups and come up with as many parameters as needed.  \n",
    "\n",
    "DL algos can be either supervised or unsupervised, and either parametric or non-parametric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chp 3: Introduction to Neural Prediction\n",
    "\n",
    "When using data to predict something, u need as many datapoints as you think the neural net needs to be accurate. For example, when trying to predict if something is in an image, you probabbly need to feed the neural net the entire image.\n",
    "\n",
    "> Always present enough information to the network, where \"enough information\" is definned loosely as how much a human might need to make the same prediction.\n",
    "\n",
    "## simplest possible neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500000000000001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the network\n",
    "weight = 0.1\n",
    "def neural_network(input, weight):\n",
    "    prediction = input * weight\n",
    "    return prediction\n",
    "\n",
    "# using the network to predict something\n",
    "number_of_toes = [8.5, 9.5, 10, 9]\n",
    "input = number_of_toes[0]\n",
    "pred = neural_network(input,weight)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to manipulate vectors is a cornerstone technique for Deep Learning. Some functions to do vector math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elementwise_multiplication(vec_a, vec_b):\n",
    "    return [vec_a[i] * vec_b[i] for i in range(len(vec_a))]\n",
    "\n",
    "def vector_sum(vec_a):\n",
    "    return sum(vec_a)\n",
    "\n",
    "def elementwise_addition(vec_a, vec_b):\n",
    "    return [vec_a[i] + vec_b[i] for i in range(len(vec_a))]\n",
    "    \n",
    "def vector_average(vec_a):\n",
    "    return sum(vec_a) / len(vec_a)\n",
    "\n",
    "a = [2,2,4]\n",
    "b = [3,3,9]\n",
    "\n",
    "# to get the dot product of a and b\n",
    "vector_sum(elementwise_multiplication(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the [wikipedia example of a dot product](https://en.wikipedia.org/wiki/Dot_product):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 3, -5]\n",
    "b = [4, -2, -1]\n",
    "vector_sum(elementwise_multiplication(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural net with 3 inputs and 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n",
      "0.98\n",
      "1.11\n",
      "1.15\n",
      "1.08\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0.1, 0.2, 0])\n",
    "def neural_network(input, weights):\n",
    "    pred = input.dot(weights)\n",
    "    return pred\n",
    "\n",
    "toes = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "wlrec = np.array([0.65, 0.8, 0.8, 0.9])\n",
    "nfans = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "\n",
    "# input corresponds to every entry for the  first game of the season\n",
    "input = np.array([toes[0], wlrec[0], nfans[0]])\n",
    "print(neural_network(input, weights))\n",
    "\n",
    "# to go through all the inputs\n",
    "for input in zip(toes,wlrec,nfans):\n",
    "    print(neural_network(np.array(input), weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural net with 3 inputs and 3 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55500000000000005, 0.98000000000000009, 0.96500000000000008]\n",
      "[0.64000000000000001, 1.1100000000000001, 1.1699999999999999]\n",
      "[0.92000000000000004, 1.1500000000000001, 1.0900000000000001]\n",
      "[0.68999999999999995, 1.0800000000000001, 1.2700000000000002]\n"
     ]
    }
   ],
   "source": [
    "weights = [[0.1, 0.1, -0.3], [0.1, 0.2, 0.0], [0.0, 1.3, 0.1]]\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    pred = vect_mat_mul(input,weights)\n",
    "    return pred\n",
    "\n",
    "def vect_mat_mul(vect,matrix):\n",
    "    out = []\n",
    "    for m in matrix:\n",
    "        out.append(np.dot(vect,m))\n",
    "    return out\n",
    "        \n",
    "toes = [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "# to go through all the inputs\n",
    "for input in zip(toes,wlrec,nfans):\n",
    "    print(neural_network(input, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a stacked neural network \n",
    "\n",
    "3 inputs and 3 outputs, with 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
