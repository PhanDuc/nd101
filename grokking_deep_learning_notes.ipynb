{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numpy is the main library used by the book\n",
    "import numpy as np \n",
    "\n",
    "# cause graphs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images-na.ssl-images-amazon.com/images/I/61gAY7APCQL._SX397_BO1,204,203,200_.jpg) \n",
    "\n",
    "This jupyter notebook contains my notes while reading [Grokking Deep Learning by Andrew Trask](https://www.manning.com/books/grokking-deep-learning). The book is sold as \"a very gentle introduction to Deep Learning\" and covers the intuition more than the theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introducing Deep Learning\n",
    "\n",
    "Deep Learning (DL) is is an intersection of Machine Learning (ML) & Artificial Intelligence (AI). This book covers the science under the hood of the major DL frameworks so you can understand whats going on when you use popular DL frameworks like Torch, Tensorflow, Keras, etc.\n",
    "\n",
    "The book covers everything past high school maths needed to grok DL. \n",
    "\n",
    "To really grok this, find a personal problem I'm interested in which to apply DL to. This could be anything where there is a dataset to predict another. Trask (the author) used Twiter to predict the stock market, which led him from barely knowing programming to a job at a hedge fund in 18 months.\n",
    "\n",
    "Some projects:\n",
    "\n",
    "- PAQI is monitoring air pollution in Pakistan. Grab a bunch of data and see if a NN can predict air pollution. Data sources: weather, factory output, what else is relevant to pollution? Target: Air pollution numbers for all the cities with pollution data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Fundamental Concepts\n",
    "\n",
    "DL uses a subset of ML methods, primarily Artificial Neural Networks. ML is a subset of CS where machines learn tasks for which they weren't explicitly progeammed for.\n",
    "\n",
    "## Supervised vs Unsupervised ml\n",
    "\n",
    "Two main types of ML:\n",
    "\n",
    "- Direct imitation, or formally **supervised ml** is basically a computer looking at a dataset A which predicts B, say weather sensor data predicting probability of rain, and trying to figure out the pattern b/w an input (sensor data) and a output set (actual weather), so when given a new input set it can apply the earlier learned pattern and come up with a prediction.\n",
    "- Indirect imitation, or fomally **unsupervised ml** looks at a not previously understoond dataset A and tries to find patterns in it. For example, it sorts data into a bunch of clusters. Clustering is the essensce of unsupervised ml. The computer doesn't know what the clusters mean but thats where the human comes in.\n",
    "\n",
    "## Parametric vs Non-Parametric Learning\n",
    "\n",
    "A parametric model has a fixed number of parameters to change, while a non-parametric model has infinite parameters.\n",
    "\n",
    "Supervised parametric dl models take in input data, process them based on a fixed number of adjustable parameters and makes a prediction. The ml model learns the optimium parameters by comparing its predictions to the actual truth, then going back and tinkering with the parameters.\n",
    "\n",
    "Unsupervised parametric dl models are similar to the supervised since they also use parameters, but they cluster the data into groups and come up with as many parameters as needed.  \n",
    "\n",
    "DL algos can be either supervised or unsupervised, and either parametric or non-parametric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chp 3: Introduction to Neural Prediction\n",
    "\n",
    "When using data to predict something, you need as many datapoints as you think the neural net needs to be accurate. For example, when trying to predict if something is in an image, you probabbly need to feed the neural net the entire image.\n",
    "\n",
    "> Always present enough information to the network, where \"enough information\" is definned loosely as how much a human might need to make the same prediction.\n",
    "\n",
    "## simplest possible neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500000000000001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the network\n",
    "weight = 0.1\n",
    "def neural_network(input, weight):\n",
    "    prediction = input * weight\n",
    "    return prediction\n",
    "\n",
    "# using the network to predict something\n",
    "number_of_toes = [8.5, 9.5, 10, 9]\n",
    "input = number_of_toes[0]\n",
    "pred = neural_network(input,weight)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is super simple - the input is being multiplied by a weight and returned. The power of NN's lies in the weights and how we update them.\n",
    "\n",
    "Now the above NN just takes in one input, but practically every thing has multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800000000000001\n",
      "1.11\n",
      "1.1500000000000001\n",
      "1.08\n"
     ]
    }
   ],
   "source": [
    "weights = [0.1, 0.2, 0]\n",
    "def neural_network(input, weights):\n",
    "    pred = w_sum(input, weights)\n",
    "    return pred\n",
    "\n",
    "toes = [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "def w_sum(a, b):\n",
    "    \"\"\"takes in 2 vectors of the same length,\n",
    "    multiples ith item of each list with each other\n",
    "    and adds them all up\n",
    "    OR - we pair up each number in the two vectors, multiply\n",
    "    every pair, then add up the results\"\"\"\n",
    "    assert(len(a) == len(b))\n",
    "    return sum([a[i]*b[i] for i in range(len(a))])\n",
    "\n",
    "for input in zip(toes, wlrec, nfans):\n",
    "    print(neural_network(input, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, being able to manipulate vectors is a cornerstone technique for Deep Learning. Some functions to do vector math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ele_mul(vec_a, vec_b):\n",
    "    assert(len(vec_a) == len(vec_b))\n",
    "    return [vec_a[i] * vec_b[i] for i in range(len(vec_a))]\n",
    "\n",
    "def vector_sum(vec_a):\n",
    "    return sum(vec_a)\n",
    "\n",
    "def ele_add(vec_a, vec_b):\n",
    "    assert(len(vec_a) == len(vec_b))\n",
    "    return [vec_a[i] + vec_b[i] for i in range(len(vec_a))]\n",
    "    \n",
    "def vector_average(vec_a):\n",
    "    return sum(vec_a) / len(vec_a)\n",
    "\n",
    "a = [2,2,4]\n",
    "b = [3,3,9]\n",
    "\n",
    "# to get the dot product of a and b\n",
    "vector_sum(ele_mul(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the [wikipedia example of a dot product](https://en.wikipedia.org/wiki/Dot_product):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my func: 3\n",
      "numpy dot func: 3\n"
     ]
    }
   ],
   "source": [
    "a = [1, 3, -5]\n",
    "b = [4, -2, -1]\n",
    "print('my func:', vector_sum(elementwise_multiplication(a,b)))\n",
    "print('numpy dot func:', np.dot(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've just made a function to do a dot product. \n",
    "\n",
    "## Weights, weights and more weights\n",
    "\n",
    "Looking at the 3 input NN above, the most imp thing is the weights:\n",
    "`weights = [0.1, 0.2, 0]`\n",
    "Crudely speaking, the weights tell us the importance of each of the 3 inputs. We can see that the third weight is zero, thus all the third inputs are essentially meningaless, since the dot product of input,weights will always make the third input zero.\n",
    "\n",
    "The position of weights is important - as NN's get more complex be careful the weights don't get moved around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy version of the above NN\n",
    "\n",
    "Numpy has fast implementations of all the vector and matrix math we'll need to do. So the below is a rewrite of the NN from above using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n",
      "1.11\n",
      "1.15\n",
      "1.08\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0.1, 0.2, 0])\n",
    "def neural_network(input, weights):\n",
    "    pred = input.dot(weights)\n",
    "    return pred\n",
    "\n",
    "toes = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "wlrec = np.array([0.65, 0.8, 0.8, 0.9])\n",
    "nfans = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "\n",
    "# input corresponds to every entry for the  first game of the season\n",
    "\n",
    "# to go through all the inputs\n",
    "for input in zip(toes,wlrec,nfans):\n",
    "    print(neural_network(np.array(input), weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights are a superpower. We can make many predictions from just a single input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.195  0.13   0.585]\n",
      "[ 0.27  0.18  0.81]\n",
      "[ 0.24  0.16  0.72]\n",
      "[ 0.24  0.16  0.72]\n"
     ]
    }
   ],
   "source": [
    "weights = [0.3, 0.2, 0.9]\n",
    "def neural_network(input, weights):\n",
    "    pred = input * np.array(weights)\n",
    "    return pred\n",
    "\n",
    "wlrec = [0.65, 0.9, 0.8, 0.8]\n",
    "\n",
    "for input in wlrec:\n",
    "    print(neural_network(input, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 3 predictions are completely seperate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural net with 3 inputs and 3 outputs\n",
    "\n",
    "Now finally we get a big more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55500000000000005, 0.98000000000000009, 0.96500000000000008]\n",
      "[0.64000000000000001, 1.1100000000000001, 1.1699999999999999]\n",
      "[0.92000000000000004, 1.1500000000000001, 1.0900000000000001]\n",
      "[0.68999999999999995, 1.0800000000000001, 1.2700000000000002]\n"
     ]
    }
   ],
   "source": [
    "          #toes #win #fans\n",
    "weights = [[0.1, 0.1, -0.3],\n",
    "           [0.1, 0.2, 0.0],\n",
    "           [0.0, 1.3, 0.1]]\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    pred = vect_mat_mul(input,weights)\n",
    "    return pred\n",
    "\n",
    "def vect_mat_mul(vect,matrix):\n",
    "    out = []\n",
    "    for m in matrix:\n",
    "        out.append(np.dot(vect,m))\n",
    "    return out\n",
    "        \n",
    "toes = [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "for input in zip(toes,wlrec,nfans):\n",
    "    print(neural_network(input, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, each output has its own set of weights which it uses to take the dot product of with the input, calculating an output.\n",
    "\n",
    "Each output node takes its own weighted sum of the input and makes a prediction.\n",
    "\n",
    "The weights used here is a list of lists of weights. This is a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the output of a neural net and use it as the input for o another net. This just means doing 2 back to back vector matrix multiplications. This is useful when the input data has patterns too conplex for a single weight matrix.\n",
    "\n",
    "## a stacked neural network \n",
    "\n",
    "3 inputs and 3 outputs, with 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.047,  1.256, -0.286])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hid = [[0.1,0.2,-0.1],\n",
    "         [-0.1,0.1,0.9],\n",
    "         [0.1,0.4,0.1]]\n",
    "w_out = [[0.3,1.1,-0.3],\n",
    "         [-0.1,0.2,0.0],\n",
    "         [0.0,1.3,0.1]]\n",
    "weights = [w_hid, w_out]\n",
    "\n",
    "def neural_net(input, weights):\n",
    "    hid = np.dot(input, weights[0])\n",
    "    # now we feed the output of the first layer into the next\n",
    "    pred = np.dot(hid, weights[1])\n",
    "    return pred\n",
    "\n",
    "toes = [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65,0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "input = list(zip(toes, wlrec, nfans))[0]\n",
    "neural_net(input,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy\n",
    "\n",
    "numpy automatically does the common math ops to numbers, vectors and matrixes. It can be a bit confusing, so try to keep the shapes of the inputs in yout head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  4,  9],\n",
       "       [ 0,  5, 12, 21]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0,1,2,3])\n",
    "b = np.array([4,5,6,7])\n",
    "c = np.array([[0,1,2,3],\n",
    "              [4,5,6,7]])\n",
    "d = np.zeros((2,4))#(2x4 matrix of zeros)\n",
    "e = np.random.rand(2,5) # random 2x5\n",
    "\n",
    "a* c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot more about numpy - see TK\n",
    "\n",
    "to sum up chp 3, neural networks perform weighted sums of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 - Gradient Descent\n",
    "\n",
    "Above, we made simple NN's which used weights to make predictions. We need to set the weight values better so the NN makes better predictions. We do this by:\n",
    "\n",
    "- predict, then compare versus the target and see how much it 'missed' by. Then update the weights so the network predicts better next time.\n",
    "\n",
    "First, we need to measure error. A popular technique is the mean squared error: `(prediction - actual) ** 2`. This amplifies big errors and makes small errors smaller. Which kinda makes sense as we want to make the NN pay more attention to big errors. We square the errors as we only care how much we missed vs negative/positive, and we avoid getting negative/postivie errors cancelling each other out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest possible way of learning weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final prediction: 0.8004999999999671 | weight: 1.6009999999999343\n"
     ]
    }
   ],
   "source": [
    "weight = 0.5\n",
    "input = 0.5\n",
    "target = 0.8\n",
    "step_amount = 0.001\n",
    "\n",
    "for iteration in range(1101):\n",
    "    pred = input * weight\n",
    "    err = (pred - target) ** 2\n",
    "    #print(f\"error {err} Preds {pred}\")\n",
    "    \n",
    "    up_pred = input * (weight + step_amount)\n",
    "    up_error = (up_pred - target) ** 2\n",
    "    \n",
    "    down_pred = input * (weight - step_amount)\n",
    "    down_error = (down_pred - target) ** 2\n",
    "    \n",
    "    if up_error < down_error:\n",
    "        weight += step_amount\n",
    "    else:\n",
    "        weight -= step_amount\n",
    "\n",
    "print(f'final prediction: {input * weight} | weight: {weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this way is inefficient and it can be impossible to predict the goal, if say we had a large step value.\n",
    "\n",
    "Another way is to calculate both the direction and amount to update the weigths from the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VOXZx/HvnclKEgIEDKsEAdkFBFlEFOoGWqXuIOKK\n1FrUatVC7WLf1/bVatVaRWotVVsldRcVxQUi7gKC7EtAloAsYUsmkP1+/zgTGEJIJmGScya5P9c1\n15zlOWd+CeTOyTNnnkdUFWOMMQ1LlNsBjDHGhJ8Vd2OMaYCsuBtjTANkxd0YYxogK+7GGNMAWXE3\nxpgGyIq7McY0QFbcjTGmAbLibowxDVC0Wy/csmVLTU9Pr9Wx+fn5JCYmhjdQmFnG4+f1fOD9jF7P\nB5axphYtWpSjqq2qbaiqrjwGDBigtTVv3rxaH1tfLOPx83o+Ve9n9Ho+VctYU8BCDaHGWreMMcY0\nQCEVdxEZJSJrRCRLRKZUsj9FRN4Wke9EZIWI3BD+qMYYY0JVbXEXER/wFDAa6AmME5GeFZr9HFip\nqn2BEcBfRCQ2zFmNMcaEKJQ3VAcBWaq6AUBEMoAxwMqgNgoki4gAScAeoKSmYYqLi8nOzqagoKDK\ndikpKaxataqmp69XXs8YHx+P889ljGmIQinu7YAtQevZwOAKbZ4EZgHbgGTgKlUtq2mY7OxskpOT\nSU9Pr7Lw5OXlkZycXNPT1ysvZ1RVdu/e7Zl3/40x4ReuWyHPB5YAPwI6Ax+KyKeqmhvcSEQmAZMA\n0tLSyMzMPOIkKSkppKam4vf7q3yx0tJS8vLywhS9bng9Y2xsLCJy1L+Bl/j9fk/nA+9n9Ho+sIx1\nJZTivhXoELTePrAt2A3Ag4HbdLJE5HugO/BNcCNVfQZ4BmDgwIE6YsSII06yatUqmjZtWm0gL18V\nl4uEjFFRUVT8N/CSzMxMT+cD72f0ej6wjHUllOK+AOgqIp1wivpY4OoKbTYDZwOfikga0A3YEM6g\nxhjjWWVlUJwPReUPv/Nc6A9sPwDF5Y+D0GEQdP5RnUaqtriraomITAbmAD5ghqquEJFbAvunA/8L\nPCciywABfqWqOXWYu874fD769OlzaH3s2LFMmXLU3Z/GmEhXWgwFuVCwDwpzneXCXCjY7xTl8gJd\n5Kfb5izYOeNw8T5if75TwGvijDvdL+4AqjobmF1h2/Sg5W3AeeGN5o6EhASWLFlSZZvS0lJ8Pt+h\n9ZKSEqKjq/9WhtrOGBOCsjIo3A8H9sDBvc7jwB6nWBfkOvsKyot1heJdkAslB6t/jahoiE2iuUZD\nSSrEJkJcEjTr4CzHJkJsUuARWI9LrrAvEWKaOI/YJhCdAFF1//lRqzQhSk9P56qrruLDDz/k3nvv\nZfr06fTr14/PPvuMcePGcdlll3HjjTeSk5NDq1at+Nvf/kbPnj25/vrriY+PZ/HixQwbNowxY8Zw\nxx13ACAizJ8/3/N988bUudISOLgH/Dshf5ezfHAvHNgbtLznyOWCfVDVTXnR8RDXFOKbQnyKs5zS\nLrAt5fC24P3xTZ3nuKZOEffFgghfNdA+d1f84e0VrNyWW+m+ilfOoerZtim/v6hXlW0OHjxIv379\nDq1PnTqVq666CoDU1FS+/fZbAKZPn05RURELFy4E4KKLLuK6667juuuuY8aMGdx777288847gHOL\n5xdffIHP5+Oiiy7iqaeeYtiwYfj9fuLj42v8dRgTEYoOQP5OyM9xCnZ54S5/+J19p+/bBpm5OB+X\nqURsEiQ0dx5NWkBKe0hocXi94nJ8ilOko+Pq9cv1Gs8Wd7dU1S1TXuQrW//yyy95/fXXAZgwYQL3\n3HPPoX1XXHHFoV9Gw4YN46677mL8+PFceumltG/fPtxfgjF1q7QY/Dsg9wfIC35sh9xtznPeD04X\nSGXimkJiS0g8AVI7syumI+269oXEVoFHS2iSGijazRp9ka4tzxb3qq6w3brNsOKHfkL9EFBwuylT\npnDhhRcye/Zshg0bxpw5c+jevXtYcxpTa2Vl4N8O+7Nh32bYvwX2bYHcrYcLd/4ujrrKjoqB5NaQ\n3AZadYPOIyEpDZJOCCragUfMkX+trsvMpF2EdXlEAs8W90hz+umnk5GRwYQJE3jxxRc5/fTTK223\nfv16+vTpQ58+fViwYAGrV6+24m7qT2kJ5GbD3o1O0d6fHSjggUK+fyuUFR95TEJzpyskuS207Q9N\n2wYKeeC5aVvnKrse3iQ0obPiXkHFPvdRo0bx4IMPVnvc3/72N2644QYefvjhQ2+oVubxxx9n3rx5\nREVF0atXL0aPHh227MYAUFIE+zbBng20y/4AZs+GPRucx77NFYq3OFfbzTpAuwHQ8yfOcsqJgef2\nzt0fJuJYca+gtLS00u0bN248Yr3iR5E7duzI3LlzD62XDz3w3HPPHdHuWEXfmBpRdbpKctbCrrXO\n8571TgHfn33oLpKuALHJ0KITtO4NPS+GFidB83RI6QBN20G0DeDaEFlxN8bLSoudLpRdayBnTaCQ\nr4Gcdc6HaMrFp0BqF+gwGPqOCxTwTny++geGnTsGbATQRseKuzFeoOrcgbJjOexYAdsDzzlrj+xG\nadoOWnaFfuOh1cnQspvzBmZiq0oLePGGTCvsjZQVd2PqW0kR7FoVVMQDhfxA0IgdTdtDWi84+bxA\nAT8ZWp5s/d8mZFbcjalLpSWwazVsWxx4fOsU8tIiZ390ApzQA7qNhtZ9nIJ+Qk/nAznGHAcr7saE\niyrszoKtiw4X8x+WHh7DJK4ptO0HQ34GbfpC61OcvvGomn/a2pjqWHE3praKD5KybwV8+i1s+Qa2\nfO2MfQLOIFFt+sLAG517w9v2DxRyuxfc1A8r7hWUD/lbUlJCjx49eP7552nSpEmtzpWZmckjjzzC\nO++8w6xZs1i5cuUxhw/et28fL730ErfeeisA27Zt4/bbb+fVV1+t9ddiwsy/EzZ/CZu/dgr5D9/R\nv/zNztSu0O0COHEwtBvovMlpV+TGRVbcKwgeW2b8+PFMnz6du+6669B+VUVViarhFdjFF1/MxRdf\nfMz9+/btY9q0aYeKe9u2ba2wuy1/N2z81Hl8/6lzCyI4ow22PRVOn8yyfU3oM/omSEx1N6sxFdjf\niFUYPnw4WVlZbNy4kW7dunHttdfSu3dvtmzZwgcffMDQoUM59dRTueKKKw7N+/r+++/TvXt3hg8f\nfmggMXA+zDR58mQAduzYwSWXXELfvn3p27cvX3zxBVOmTGH9+vX069ePe+65h40bN9K7d28ACgoK\nuOGGG+jTpw/9+/dn3rx5h8556aWXMmrUKLp27cq9994LOB/Euv766+nduzd9+vThscceq89vW+Q6\nuBdWvQPv/QqmnQ4PnwSvXAdLZjqf1jznDzDxY5iyBW58D865n90tB1lhN54U0pW7iIwC/oozE9Oz\nqvpghf33AOODztkDaKWqe2qd7L0psH1ZpbsSSkvAV4s/Olr3gdHVDyUAzsQa7733HqNGjQJg3bp1\nPP/88wwZMoScnBweeOABPvroIxITE3nooYd49NFHuffee7n55puZO3cuaWlpTJw4sdJz33777Zx1\n1lm88cYblJaW4vf7efDBB1m+fPmhvxqCPxH71FNPISIsW7aM1atXc95557F27VoAlixZwuLFi4mL\ni6Nbt27cdttt7Ny5k61bt7J8+XLA+avAVKKsFLZ+C1kfOY+tiwB17mA5cTD0/i10OtPpL/fFuJ3W\nmBqptkKKiA94CjgXyAYWiMgsVV1Z3kZVHwYeDrS/CLjzuAq7i4LHlhk+fDg33XQT27Zto2PHjgwZ\nMgSAr776ipUrVzJs2DAAioqKGDp0KKtXr6ZTp0507dqVvLw8rrnmGp555pmjXmPu3Lm88MILgNPH\nn5KSwt69e4+Z6bPPPuO2224DoHv37nTs2PFQcT/77LNJSUkBoGfPnmzatIlevXqxYcMGbrvtNi68\n8ELOO69BTJIVHnnbYf1cWPchbJjnXK0jzrgqZ/3KKebtB9owsybihXL5OwjIUtUNACKSAYwBVh6j\n/Thg5nEnq+IK+2AdDvl7rPHcg4ftVVXOPfdcZs488susbnq+uhAXd7gI+Xw+SkpKaN68Od999x1z\n5sxh+vTpvPzyy8yYMaPes3mCKvywBFbPhrXvHf5rMPEEOHk0dDnbmcvS7is3DUwofe7tgC1B69mB\nbUcRkSbAKOC144/mXUOGDOHzzz8nKysLgPz8fNauXUv37t3ZuHEj69evBziq+Jc7++yzefrppwGn\nf3z//v0kJycfGmysouHDh/Piiy8CsHbtWjZv3ky3bt2OmS8nJ4eysjIuu+wyHnjggUOzRzUapcWw\nfh68ezc81gueGQGfPuLM6HP27+Cnn8Iv18AlT0Ofy62wmwYp3HfLXAR8fqwuGRGZBEwCSEtLO2pk\nxZSUlGMWuGClpaUhtautiuf2+/2UlZUd2h4fH8+0adO48sorKSpyPmn429/+ljZt2vD4448zevRo\nEhISOP3009m7dy95eXkUFBRQVFREXl4ef/zjH7n99tv5xz/+gc/n49FHH2Xw4MEMGjSInj17cu65\n53LzzTcfes0JEyZw55130qtXL6Kjo5k2bRpFRUVHnBOc9wkOHDjA2rVrufXWWykrc0YG/P3vf1/p\n90tVj/o38BK/3x9yvqjSAlJ3L6Jlzlek7l5EdGk+pVGx7G3en5xul7E7dSDFsSlQCqzZA2vm13tG\nN3g9H1jGuiKqx5i3sLyByFDgflU9P7A+FUBV/6+Stm8Ar6jqS9W98MCBA7V8/tFyq1atokePHtWG\ndmsmppqIhIyLFy+mf//+bsc4pszqJiUuLoCsD2H567D2fSg+4Ewa0e0C6H4BnDTSmW3ezYwu83o+\nsIw1JSKLVHVgde1CuXJfAHQVkU7AVmAscHUlL5gCnAVcU8OsxoSupMh5I3T567D6XSjKc+bbPOUq\n6H0pnHh67e6kMqaBqfanQFVLRGQyMAfnVsgZqrpCRG4J7J8eaHoJ8IGq5tdZWtM4qUL2AljyIqx4\nEwr2OeOX9xzjFPROZ9qtisZUENIljqrOBmZX2Da9wvpzwHPHG0hVERt/us5V1x3nBXEFu2D+I7Dk\nJWeWoZgm0P1C6H25c4eLzSBkzDF56u/X+Ph4du/eTWpqqhX4OqSq7N69+5hTCrqq6ACsehu+e4kh\nGz4BFDoOg+F3OVfqNp65MSHxVHFv37492dnZ7Nq1q8p2BQUFxMfH11Oq2vF6xvj4ePLzPdSDtnM1\nLJwB32VA4X5odiKbOl5F+pgpzvyfxpga8VRxj4mJoVOn6n+QMzMzPX2XB0RGxk2bNrkboKTQuUpf\nOAM2fQ6+WOfq/NTroOMwNs6fT7oVdmNqxVPF3TQSezc5BX3xf5yp5ZqnO4Ny9b8GElu6nc6YBsGK\nu6kfqs4Y6F8+BavfAcSZWm7gjc796DaJhTFhZcXd1K3SYlj5llPUt30L8c1g2B1w2s2QUukoFsaY\nMLDibupGwX5Y+C/45hnI3QotOsMFj0C/qyE2sfrjjTHHxYq7Ca/83fDVNPjmH85dL53OhAsfha7n\nWdeLMfXIirsJj9xt8MXfYNFzUHwQelwEw38Jbfu5ncyYRsmKuzk+ezfCZ485nyItK4U+VzgfOGp1\n7CGJjTF1z4q7qZ39W2H+w7D43yBR0G+880ap3ZdujCdYcTc149/lXKkveBa0DAZc73S/NG3rdjJj\nTBAr7iY0B/c5fepfPQ0lB6HvOGfO0eYd3U5mjKmEFXdTtZJC586X+X92bm/sdQmM+DW0OtntZMaY\nKlhxN5VThVWz4MPfw97vofPZcM790OYUt5MZY0Jgxd0cbesimHMfbP4SWvWAa16DLue4ncoYUwNW\n3M1hudvgo/th6X8hsRX8+HHoP8GmrTMmAoX0Uysio4C/4kyz96yqPlhJmxHA40AMkKOqZ4Uxp6lL\npcXw9d8h8/+c5eG/hDPutIkxjIlg1RZ3EfEBTwHnAtnAAhGZpaorg9o0A6YBo1R1s4icUFeBTZht\n/Bxm3w07V0LX82H0Q3avujENQChX7oOALFXdACAiGcAYYGVQm6uB11V1M4Cq7gx3UBNmeTvovuox\nyMyElBNh7EzofoHbqYwxYSLVTZQsIpfjXJFPDKxPAAar6uSgNuXdMb2AZOCvqvpCJeeaBEwCSEtL\nG5CRkVGr0H6/n6SkpFodW188m1HLaPPDh3Re/xxSVsSWEy9l84mXU+aLczvZUTz7PQzi9YxezweW\nsaZGjhy5SFUHVtcuXO+URQMDgLOBBOBLEflKVdcGN1LVZ4BnAAYOHKgjRoyo1YtlZmZS22Priycz\n7l4Pb98BGz+F9OF8fcI4Bl8wnnS3cx2DJ7+HFXg9o9fzgWWsK6EU961Ah6D19oFtwbKB3aqaD+SL\nyHygL7AW477SEmcY3nl/dOYpvegJOPVaDn7yidvJjDF1JJTivgDoKiKdcIr6WJw+9mBvAU+KSDQQ\nCwwGHgtnUFNL25fDrMmwbTF0uxAufMTGgTGmEai2uKtqiYhMBubg3Ao5Q1VXiMgtgf3TVXWViLwP\nLAXKcG6XXF6XwU01ykqdsWDmPgAJzeCK56DnT0DE7WTGmHoQUp+7qs4GZlfYNr3C+sPAw+GLZmpt\n7yZ44xbY/AX0uNj5MFJiqtupjDH1yD562JCoOpNmvPcrZ/0n06HvWLtaN6YRsuLeUOTvhrdvh9Xv\nQMdh8JOnbTheYxoxK+4NwaYv4NWb4EAOnPu/MPTnEOVzO5UxxkVW3CNZWRl89heY9ydong4TP4I2\nfd1OZYzxACvukcq/E16fBBvmQe/LnDdN45u6ncoY4xFW3CPR9/PhtYnOzEgX/RVOvc7eNDXGHMGK\neyRRhS+ecMZcT+0CE96AtF5upzLGeJAV90hRlA9v/RxWvAE9x8CYaRDnjYGMjDHeY8U9EuzZABnX\nwK5VcM4fYNgd1g1jjKmSFXevW/cRvHYjIDD+VehyttuJjDERIMrtAOYYVOHzJ+DFyyGlA0zKtMJu\njAmZXbl7UWkxvPtL+PZ5p3/9J09DbKLbqYwxEcSKu9cc3AcvXwvff+JMVD3yNxBlf2AZY2rGiruX\n7PkeXrrSeR4zDfqPdzuRMSZCWXH3is1fQ8Y4Zxz2a9+E9DPcTmSMiWBW3L1gzXvwyvXODElXvwIt\nu7idyBgT4ULqzBWRUSKyRkSyRGRKJftHiMh+EVkSePwu/FEbqMX/gYzxcEJPuOlDK+zGmLCo9spd\nRHzAU8C5OBNhLxCRWaq6skLTT1X1x3WQsWFShc8eg4//AJ1/BFf+2z5xaowJm1Cu3AcBWaq6QVWL\ngAxgTN3GauDKymDOr53C3vtyGPdfK+zGmLASVa26gcjlwChVnRhYnwAMVtXJQW1GAK/jXNlvBe5W\n1RWVnGsSMAkgLS1tQEZGRq1C+/1+kpK8XQyPlVHKSui++gnSdn5Cdrsfk9XlJhB3bnX0+vfR6/nA\n+xm9ng8sY02NHDlykaoOrLahqlb5AC4Hng1anwA8WaFNUyApsHwBsK668w4YMEBra968ebU+tr5U\nmrG4UHXm1aq/b6r6ycOqZWX1niuY17+PXs+n6v2MXs+nahlrClio1dRXVQ2pW2Yr0CFovX1gW/Av\niFxV9QeWZwMxItIyhHM3HsUF8N9rnDlORz0EZ95tg38ZY+pMKMV9AdBVRDqJSCwwFpgV3EBEWos4\nlUpEBgXOuzvcYSNW0QGYORbWfeDMmDTkFrcTGWMauGrvllHVEhGZDMwBfMAMVV0hIrcE9k/H6br5\nmYiUAAeBsYE/H0xhHrx0FWz+En4yDfpd7XYiY0wjENKHmAJdLbMrbJsetPwk8GR4ozUABbnwn8tg\n6yK49B/Q53K3ExljGgn7hGodiSotcMaJ2fYtXPEc9LzY7UjGmEbEintdKD5In2V/hP3L4fIZVtiN\nMfXOxpINt5JCyBhPs33L4JK/Q69L3E5kjGmErLiHU0kRvHwdrP+YNd0mwylXup3IGNNIWbdMuJSW\nwOsTYe17cOFf2J7fhe5uZzLGNFp25R4OqvDOHbDyLTj/T3DaRLcTGWMaOSvu4fDx/zhD9555Lwz9\nudtpjDHGivtx+3IafPYoDLgeRv7a7TTGGANYcT8+S1+GOVOhx0Vw4aM2VowxxjOsuNfWuo/gzZ9B\n+nC49FmI8rmdyBhjDrHiXhvbFsPLE5yp8ca+BDHxbicyxpgjWHGvqf3Z8NJYaJIK41+F+KZuJzLG\nmKPYfe41UZALL14JxQdgwhxITnM7kTHGVMqKe6hKS+DVG2DXarjmVUjr6XYiY4w5JivuoVCF9+6B\nrI/gor9C5x+5ncgYY6pkfe6h+PIpWDgDht3h3M9ujDEeF1JxF5FRIrJGRLJEZEoV7U4TkRIRaTiz\nUqz7CD74DfS4GM6+3+00xhgTkmqLu4j4gKeA0UBPYJyIHNXhHGj3EPBBuEO6Zvd6ePVGSOsNl0yH\nKPtDxxgTGUKpVoOALFXdoKpFQAYwppJ2twGvATvDmM89Bbkwcxz4omHsixCb6HYiY4wJmVQ3j3Wg\ni2WUqk4MrE8ABqvq5KA27YCXgJHADOAdVX21knNNAiYBpKWlDcjIyKhVaL/fT1JSUq2ODYmW0Xv5\nn0jdvYjv+v4P+5r3qfEp6jxjGHg9o9fzgfczej0fWMaaGjly5CJVHVhtQ1Wt8gFcDjwbtD4BeLJC\nm1eAIYHl54DLqzvvgAEDtLbmzZtX62ND8vEDqr9vqvrV9Fqfos4zhoHXM3o9n6r3M3o9n6plrClg\noVZTX1U1pFshtwIdgtbbB7YFGwhkiDNwVkvgAhEpUdU3Qzi/t6ycBfP/DP2ugUGT3E5jjDG1Ekpx\nXwB0FZFOOEV9LHB1cANV7VS+LCLP4XTLRF5hz8mCN2+FdgPhwr/YKI/GmIhVbXFX1RIRmQzMAXzA\nDFVdISK3BPZPr+OM9aPoALx8Lfhi4MrnbTAwY0xEC+kTqqo6G5hdYVulRV1Vrz/+WC6YfQ/sXOkM\nBpbS3u00xhhzXOzGbXCmyFvyHzjzbuh6jttpjDHmuFlx374c3v0ldDoTRkx1O40xxoRF4y7uBblO\nP3t8M7jsnzabkjGmwWi8o0Kqwju/gL0b4bq3IekEtxMZY0zYNN4r9+9mwvLXYORUSB/mdhpjjAmr\nxlncd6+Hd++GjmfAGXe5ncYYY8Ku8RX3kiJ47SbnfvZL/2797MaYBqnx9bln/gm2LYYrX7D72Y0x\nDVbjunL/fj589jiceh30rGzUYmOMaRgaT3E/sAde/ymkdoFR/+d2GmOMqVONp1tm9t2QvwvGzbSJ\nN4wxDV7juHJf8YZz2+OIX0Hbfm6nMcaYOtfwi7t/J7xzF7Q9FYbd6XYaY4ypFw27uKvC23dAUb4z\nwbWv8fRCGWMat4Zd3L/LgDWz4ezfQatubqcxxph603CL+/6t8N6v4MShMORnbqcxxph6FVJxF5FR\nIrJGRLJEZEol+8eIyFIRWSIiC0XkjPBHrQFVmHUblBXDT6bZp1CNMY1OtZ3QIuIDngLOBbKBBSIy\nS1VXBjX7GJilqioipwAvA93rIjCAMwF4Fb6bCes/hgsegRYn1VUMY4zxrFCu3AcBWaq6QVWLgAzg\niI93qqpfD1fcRKCa6lt7n63L4b7PD7Inv6jyBv5dMOfX0GEIDLyprmIYY4ynhVLc2wFbgtazA9uO\nICKXiMhq4F3gxvDEO1pa0zh+8CtPfLyu8gbvT3Hujrn4CYhquG8pGGNMVaS6Lg4RuRwYpaoTA+sT\ngMGqOvkY7c8EfqeqR01GKiKTgEkAaWlpAzIyMmoV+h9L/Hy1Q/jjGQm0TjxcwFvsXsgpy/6X79PH\nsSl9bK3OHS5+v5+kpCRXM1TH6xm9ng+8n9Hr+cAy1tTIkSMXqerAahuqapUPYCgwJ2h9KjC1mmM2\nAC2rajNgwACtrTfe/1h7/vY9nfTCgsMbC3JV/9JT9cnBqsWFtT53uMybN8/tCNXyekav51P1fkav\n51O1jDUFLNRq6raqhtQtswDoKiKdRCQWGAvMCm4gIl1ERALLpwJxwO4QfxHVWLO4KG45qzNzVuzg\nm+/3OBvnPgC5W53umOjYunppY4yJCNUWd1UtASYDc4BVwMuqukJEbhGRWwLNLgOWi8gSnDtrrgr8\nhqkzE4efROum8fzx3ZWUbf4Gvv47DLoZOgyqy5c1xpiIENLn8VV1NjC7wrbpQcsPAQ+FN1rVEmJ9\n3H1+N371yrfkvfoAKU3bOp9ENcYYE9lD/l7avx375j5OSu4aDl7yPAlxyW5HMsYYT4joewWj8ndw\nQ9FMMsv68pfNXd2OY4wxnhHRxZ0PfoOvrJiFPabyry83sWZ7ntuJjDHGEyK3uH8/H5a9Amf8gpsu\nPpvk+Gh++9by6ocmMMaYRiAii7uUFcO7d0OzjnDGnTRPjOVXo7rzzfd7eHPJVrfjGWOM6yKyuLfP\nfhty1sDoP0NMAgBXDexAvw7NeOCdVcced8YYYxqJyCvu+7NJ3/hf6HYBdBt1aHNUlPDQZaeQW1DM\nH95e4WJAY4xxX+QV922LKYuKgVEPHrWrW+tkfj6yC28t2cbHq3a4EM4YY7wh8op7j4v4cuiz0Lxj\npbtvHdGF7q2Tue+N5eQWFNdzOGOM8YbIK+5AmS/+mPtio6N46LJT2JlXwJ/eXVWPqYwxxjsisrhX\np2+HZtw8/CQyFmyx7hljTKPUIIs7wF3nnUz31snc++pSduUVuh3HGGPqVYMt7nHRPp4Y15+8whJ+\n9dpS+3CTMaZRabDFHeDktGSmju7O3NU7+c/Xm92OY4wx9aZBF3eA609P58yTW/HHd1eydoeNPWOM\naRwafHEXER654hSS42O45T+L8BeWuB3JGGPqXEjFXURGicgaEckSkSmV7B8vIktFZJmIfCEifcMf\ntfZOSI7nb+P6szEn3/rfjTGNQrXFXUR8OFPnjQZ6AuNEpGeFZt8DZ6lqH+B/gWfCHfR4DTkplXvO\n7867S3/g+S82uh3HGGPqVChX7oOALFXdoKpFQAYwJriBqn6hqnsDq18B7cMbMzx+euZJnNPjBP44\nexXfbt6RydsNAAAUV0lEQVRb/QHGGBOhQinu7YAtQevZgW3HchPw3vGEqitRUcJfruhH65R4bvn3\nIn7Yf9DtSMYYUyekuv5nEbkcGKWqEwPrE4DBqjq5krYjgWnAGaq6u5L9k4BJAGlpaQMyMjJqFdrv\n95OUlFSrYwGy88p44KuDpCVG8etB8cRFS63PdSzHm7E+eD2j1/OB9zN6PR9YxpoaOXLkIlUdWG1D\nVa3yAQwF5gStTwWmVtLuFGA9cHJ151RVBgwYoLU1b968Wh9b7uNV2zV9yjt6y78Xamlp2XGfr6Jw\nZKxrXs/o9Xyq3s/o9XyqlrGmgIUaQo0NpVtmAdBVRDqJSCwwFpgV3EBETgReByao6tpQfwO56Ufd\n07jvgh68t3w7j34YEZGNMSZk0dU1UNUSEZkMzAF8wAxVXSEitwT2Twd+B6QC00QEoERD+bPBZTed\n0Yl1O/w8OS+Lds0TGDfoRLcjGWNMWFRb3AFUdTYwu8K26UHLE4GJ4Y1W90SEBy7pzfbcAu57Yxkt\nEmM5v1drt2MZY8xxa/CfUK1OjC+Kp685lVPaN+O2mYv5esNR7wMbY0zEafTFHaBJbDT/uv40OjRP\nYOILC1m5LdftSMYYc1ysuAc0T4zlhZsGkxQXzTX//JrV263AG2MilxX3IO2aJTDz5iHE+ISr//E1\na7bbKJLGmMhkxb2C9JaJZEwaSnSUcPU/vrJhgo0xEcmKeyU6tUxk5qQh+AIF3vrgjTGRxor7MXRu\nlcTMSUOI8UVx1TNfsmDjHrcjGWNMyKy4V6FzqyRe/dnptEqO45pnv2bu6h1uRzLGmJBYca9Gu2YJ\nvPLToZyclszNLyzitUXZbkcyxphqWXEPQWpSHDMnDWFwpxb88pXvePSDNZSV2WxOxhjvsuIeoqS4\naJ67YRBXDmzPE3OzuG3mYg4WlbodyxhjKhXS2DLGERsdxUOXnULnVkk8+P5qsvce4JlrB5LWNN7t\naMYYcwS7cq8hEeGnZ3Xm79cMYN1OPxc+8SlfrM9xO5YxxhzBinstnderNW/+fBhNE2K45tmveWpe\nlvXDG2M8w4r7cTg5LZlZk8/ggj5teHjOGia+sJC9+UVuxzLGGCvuxyspLpq/jevP/4zpxafrdnH+\n4/PJXLPT7VjGmEYupOIuIqNEZI2IZInIlEr2dxeRL0WkUETuDn9MbxMRrh2azps/H0azJjFc/68F\nvLCy0O6mMca4ptriLiI+4ClgNNATGCciPSs02wPcDjwS9oQRpFfbFGZNPoOJZ3Ri7uYSLnziUxZt\nsmELjDH1L5Qr90FAlqpuUNUiIAMYE9xAVXeq6gKguA4yRpT4GB+/+XFP7j0tnsKSMi57+kvue2MZ\n+w82+m+NMaYehVLc2wFbgtazA9tMFXqm+vjgzjO5cVgnZn6zmXMe/YR3l/6Aqt1RY4ype1JdsRGR\ny4FRgUmwEZEJwGBVnVxJ2/sBv6pW2j0jIpOASQBpaWkDMjIyahXa7/eTlJRUq2PrS3DGjftL+deK\nIjblltErNYpx3eNon+z+e9le/z56PR94P6PX84FlrKmRI0cuUtWB1TZU1SofwFBgTtD6VGDqMdre\nD9xd3TlVlQEDBmhtzZs3r9bH1peKGYtLSnXGZxv0lPvnaKcp7+ivX1+qOXkF7oQL8Pr30ev5VL2f\n0ev5VC1jTQELNYQaG8rl4wKgq4h0EpFYYCwwqza/cRqzaF8UNwzrRObdI7h2aDoZC7Yw4uFMpmVm\ncaCoxO14xpgGptrirqolwGRgDrAKeFlVV4jILSJyC4CItBaRbOAu4Dciki0iTesyeKRqnhjL/Rf3\nYs4vzuS0Ti348/trOPPP83j20w0UFNutk8aY8Ahp4DBVnQ3MrrBtetDydqB9eKM1bF1OSGLG9aex\naNMeHvtwHQ+8u4pn5m/g1hGdueq0E0mI9bkd0RgTwdx/V6+RG9CxBf+ZOJiMSUNIb5nI/W+vZNhD\nc3nsw7Xs9he6Hc8YE6GsuHvEkJNS+e+kIfx30hBOPbEZf/14Hac/OJf73ljGhl1+t+MZYyKMjefu\nISLC4JNSGXxSKlk78/jH/O95ZWE2L369mTO6tGT84BM5p2caMT77nWyMqZoVd4/qckIyD11+Cr88\n/2T++80WZn6zmZ+9+C2tkuO4amAHrjqtAx1aNHE7pjHGo6y4e9wJyfHcdnZXbh3ZhU/W7uTFrzbz\nVGYWT87L4rT05vykfzsu7NOGZk1i3Y5qjPEQK+4Rwhcl/Kh7Gj/qnsbWfQd5c/FW3li8lfveWM79\ns1YwotsJXNy3LSO6tSI5PsbtuMYYl1lxj0DtmiXw85FduHVEZ1Zsy+WtJVt5a8k2Ply5g1hfFKd3\nSeX8Xq05p0carZLj3I5rjHGBFfcIJiL0bpdC73YpTBndg28372XO8u3MWbmdqa8v49eyjFNPbM7I\nbq0Y3rUVvdul4IsSt2MbY+qBFfcGwhclnJbegtPSW3DfhT1YvT2POSu289GqHTzywVoe+WAtzZrE\nMKxLS87s2pIzurZyO7Ixpg5ZcW+ARIQebZrSo01TfnHOyeT4C/k8K4f5a3P4dN0u3l36AwAtE4Th\nO5YwML05g9Jb0LlVElF2ZW9Mg2DFvRFomRTHmH7tGNOvHarK2h1+Ps/KYfaCNXy6Loc3Fm8FoFmT\nGAZ2bE6/Ds3o3S6FPu1SSE2yPntjIpEV90ZGROjWOplurZM5qWQTZ511Fpt2H2DBxj0s3LiXBZv2\n8NGqwxN8t2uWQO92TTmlfTN6tW1K99ZNSWsah4hd4RvjZVbcGzkRIb1lIuktE7liYAcAcguKWbE1\nl+Vb97N0636Wb93PnBU7Dh2THB/NyWnJnJyWFHhOpmtaEq2SrOgb4xVW3M1RmsbHMLRzKkM7px7a\nlltQzMptuazbkcfaHX7W7Mjj/eXbmfnN4RkYk+Oi6diyCR1bJNIxtQnpqYHnlomckGyF35j6ZMXd\nhKRpfAxDTkplyEmHC76qkuMvYt2OPNbsyGPT7gNs3J3Pyh9ymbNiOyVlh6dwjI+Jom2zBNqmJNAm\nJZ42zRJo1yyeNikJtA08J8bZf0djwsV+mkytiQitkuNolRzH6V1aHrGvpLSMbfsK2Lg7n017DrAp\nJ59t+w+ybV8Bn6zdxS5/IRWn702Oj6ZVchwtk5xzFu4rZHnZukPrLZPiaJkcR2piLPExNt69MVUJ\nqbiLyCjgr4APeFZVH6ywXwL7LwAOANer6rdhzmoiSLQvihNTm3BiauWDmxWVlLEjt4Af9hfwQ6Do\nb99/kF3+QnLyili1LZcf9pXw0ea1lR4fHxNFs4RYmjWJISUhhmZNYg6vBy8nxJAUF01iXDRJcdEk\nxUfTJMZnt3yaBq/a4i4iPuAp4FwgG1ggIrNUdWVQs9FA18BjMPB04NmYSsVGR9GhRZMqR7bMzMxk\nyLDh5PgLyfEXkZNXyC5/IXvyi9h/sJh9B4rYd6CYfQeL2ZhzgH0H97H3QDFFJWVVvrYIJMZGkxjn\nIzEumuRA8S9fbhLnIz7aR0Ksj/iY8kcUCTE+EmKO3LYlr4yNOflO22gfcTFRxPqi7JeHcV0oV+6D\ngCxV3QAgIhnAGCC4uI8BXgjMzP2ViDQTkTaq+kPYE5tGJT7GR/vmTWjfPPThjQuKSwNF3yn++YUl\n+AtLyC8sxV9YjL+wFH9ByaHtzr4S9uQfwF9YwoGiUgqKSzlYXHpU11GlPs88alN0lBDjiyLGJ8RG\n+4j1CTHRTuGP8UUFloXY6MC6L4rYQ/vl0DZflBx6RJcvi+Dzla9H4RPw+aKO2B/tc5ZXby+haMX2\nwLrTJiqwP0qcrrUocZajRJAKz8dqI5Ucc2T76tuYuhVKcW8HbAlaz+boq/LK2rQDrLibehcf46N1\nio/WKfHHdR5VpbCkjMLiMg4WHy745c+FxWUsXLKUzid3D2wvo6C4lOLSMopLyygqKaO4VCk6tFy+\n3dlWHDi3v6CEwkP7laKSMopKyygtU0rLlJKyw8tlofyyqWjJouP6PtQVESgv8fLBbCSwDUA4vFMO\ntZWjj5PA1iPaHrvd4d8pEvRaR75uxeMACgoKSPh6bpXtgk5NJYtHnG/saR2YOPykKr8/x6te31AV\nkUnAJIC0tDQyMzNrdR6/31/rY+uLZTx+Xs8nQI+kApJys47cGB141OrDvYLz1lblbxiXqVPgyx+l\nh54VDVov3+fPP0BcfAJlQFlZcHsA51wKaOC5TA8vH9quUIbzy658W1ngmfJjjmpbfg6tfHtgHYWi\n4iJiYg4PU13e5tB60Gs5Zzy8ozxn8LFHtCnfr4e3KYcPqvz4o1+3OLaM6OjiwwkqZgw637EE79u5\nZQOZmZuraB0GqlrlAxgKzAlanwpMrdDm78C4oPU1QJuqzjtgwACtrXnz5tX62PpiGY+f1/Opej+j\n1/OpWsaaAhZqNXVbVUOaIHsB0FVEOolILDAWmFWhzSzgWnEMAfar9bcbY4xrqu2WUdUSEZkMzMH5\nW3GGqq4QkVsC+6cDs3Fug8zCuRXyhrqLbIwxpjoh9bmr6mycAh68bXrQsgI/D280Y4wxtRVKt4wx\nxpgIY8XdGGMaICvuxhjTAFlxN8aYBsiKuzHGNECiIQ2eUQcvLLIL2FTLw1sCOWGMUxcs4/Hzej7w\nfkav5wPLWFMdVbVVdY1cK+7HQ0QWqupAt3NUxTIeP6/nA+9n9Ho+sIx1xbpljDGmAbLibowxDVCk\nFvdn3A4QAst4/LyeD7yf0ev5wDLWiYjsczfGGFO1SL1yN8YYU4WIK+4iMkpE1ohIlohMcSlDBxGZ\nJyIrRWSFiNwR2N5CRD4UkXWB5+ZBx0wNZF4jIufXY1afiCwWkXe8ljEwHeOrIrJaRFaJyFAv5Qu8\n5p2Bf+PlIjJTROLdzigiM0Rkp4gsD9pW40wiMkBElgX2PSFhmvvuGPkeDvw7LxWRN0SkmVv5jpUx\naN8vRURFpKWbGY9bKIO+e+WBM+TweuAkIBb4DujpQo42wKmB5WRgLdAT+DMwJbB9CvBQYLlnIGsc\n0CnwNfjqKetdwEvAO4F1z2QEngcmBpZjgWYey9cO+B5ICKy/DFzvdkbgTOBUYHnQthpnAr4BhuBM\n//QeMLoO850HRAeWH3Iz37EyBrZ3wBnefBPQ0s2Mx/uItCv3Q5N1q2oRUD5Zd71S1R9U9dvAch6w\nCqcQjMEpWASefxJYHgNkqGqhqn6PM+79oLrOKSLtgQuBZ4M2eyKjiKTg/ID9E0BVi1R1n1fyBYkG\nEkQkGmgCbHM7o6rOB/ZU2FyjTCLSBmiqql+pU6VeCDom7PlU9QNVLQmsfgW0dyvfsTIGPAbcy5Gz\n4rmS8XhFWnE/1kTcrhGRdKA/8DWQpodnoNoOpAWW3cr9OM5/1LKgbV7J2AnYBfwr0G30rIgkeigf\nqroVeATYjDPZ+35V/cBLGYPUNFO7wHLF7fXhRpyrXPBQPhEZA2xV1e8q7PJMxpqItOLuKSKSBLwG\n/EJVc4P3BX6Tu3Yrkoj8GNipqouO1cbljNE4fxY/rar9gXyc7oRDPPA9bI5z1dYJaAskisg1wW3c\nzlgZL2YqJyL3ASXAi25nCSYiTYBfA79zO0u4RFpx34rTJ1aufWBbvRORGJzC/qKqvh7YvCPwpxqB\n552B7W7kHgZcLCIbcbqvfiQi//FQxmwgW1W/Dqy/ilPsvZIP4Bzge1XdparFwOvA6R7LWK6mmbZy\nuGskeHudEZHrgR8D4wO/gLyUrzPOL/HvAj8z7YFvRaS1hzLWSKQV91Am665zgXfE/wmsUtVHg3bN\nAq4LLF8HvBW0fayIxIlIJ6ArzhsxdUZVp6pqe1VNx/k+zVXVa7ySUVW3A1tEpFtg09nASq/kC9gM\nDBGRJoF/87Nx3l/xUsZyNcoU6MLJFZEhga/t2qBjwk5ERuF0EV6sqgcq5HY9n6ouU9UTVDU98DOT\njXPTxHavZKwxt9/RrekDZyLutTjvWN/nUoYzcP7sXQosCTwuAFKBj4F1wEdAi6Bj7gtkXkM9v6MO\njODw3TKeyQj0AxYGvo9vAs29lC/wmn8AVgPLgX/j3DHhakZgJs57AMU4Reim2mQCBga+rvXAkwQ+\n1FhH+bJw+q3Lf16mu5XvWBkr7N9I4G4ZtzIe78M+oWqMMQ1QpHXLGGOMCYEVd2OMaYCsuBtjTANk\nxd0YYxogK+7GGNMAWXE3EUtEvgg8p4vI1WE+968rey1jIoXdCmkinoiMAO5W1R/X4JhoPTyQVWX7\n/aqaFI58xrjBrtxNxBIRf2DxQWC4iCwRZ/x1X2D88AWB8cN/Gmg/QkQ+FZFZOJ+GRUTeFJFF4ozZ\nPimw7UGckSCXiMiLwa8ljofFGd99mYhcFXTuTDk8Pv2L5WN7i8iD4oz9v1REHqnP75FpvKLdDmBM\nGEwh6Mo9UKT3q+ppIhIHfC4iHwTangr0VmfoVoAbVXWPiCQAC0TkNVWdIiKTVbVfJa91Kc4na/sC\nLQPHzA/s6w/0whkW+HNgmIisAi4BuquqStAkFcbUJbtyNw3RecC1IrIEZyjmVJzxQMAZE+T7oLa3\ni8h3OGOMdwhqdyxnADNVtVRVdwCfAKcFnTtbVctwPmKfDuwHCoB/isilwIFKzmlM2FlxNw2RALep\nar/Ao5M647CDM7Sw08jpqz8HGKqqfYHFQPxxvG5h0HIpzsxDJTgTdryKMyLi+8dxfmNCZsXdNAR5\nONMdlpsD/CwwLDMicnJgIpCKUoC9qnpARLrjTJdWrrj8+Ao+Ba4K9Ou3wplN6pgjPwbG/E9R1dnA\nnTjdOcbUOetzNw3BUqA00L3yHPBXnC6RbwNvau6i8unP3gduCfSLr8Hpmin3DLBURL5V1fFB298A\nhuLMqanAvaq6PfDLoTLJwFsiEo/zF8VdtfsSjakZuxXSGGMaIOuWMcaYBsiKuzHGNEBW3I0xpgGy\n4m6MMQ2QFXdjjGmArLgbY0wDZMXdGGMaICvuxhjTAP0/x5oeA8zJqMgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1065e29b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight = 0.5\n",
    "input = 0.5\n",
    "target = 0.8\n",
    "alpha = .01\n",
    "\n",
    "predictions, errors = [], []\n",
    "for i in range(1500):\n",
    "    pred = input * weight\n",
    "    error = (pred - target) ** 2\n",
    "    \n",
    "    # delta measures by how much the target was missed\n",
    "    delta = pred - target\n",
    "    weight_delta = delta * input\n",
    "    weight = weight - weight_delta * alpha\n",
    "    predictions.append(pred)\n",
    "    errors.append(error)\n",
    "    \n",
    "    #if i % 220 == 0:\n",
    "    #    print(f'Step {i:2d} Error {error:f} Prediction {pred:f}')\n",
    "\n",
    "plt.xlabel('iterations')\n",
    "plt.grid()\n",
    "plt.plot(errors, label=\"Errors\")\n",
    "plt.plot(predictions, label=\"Predictions\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a neural net with descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 70.0000007766\n",
      "weights: [ 103.47539901   43.9901596     1.4099016 ]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([0.25,1.0,0.1])\n",
    "target = 70 # my weight\n",
    "weights = np.array([1,3,1])\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(40):\n",
    "    pred = np.dot(inputs, weights)\n",
    "    error = (pred - target)**2\n",
    "    delta_error = input * (pred - target)\n",
    "    weights = weights - (delta_error * learning_rate)\n",
    "print('pred:', pred)\n",
    "print('weights:', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Learning Multiple Weights at a Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with Multiple Inputs & Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.3549355,  1.524418 , -0.624401 ]), [0.0, 1, 0.0])"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: empty network\n",
    "weights = [ [0.1, 0.1, -0.3], #hurt\n",
    "           [0.1, 0.2, 0.0], #win\n",
    "           [0.0, 1.3, 0.1] ]#sad?\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    pred = np.dot(input, weights)\n",
    "    return pred\n",
    "\n",
    "#Predict\n",
    "toes = [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65,0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "hurt = [0.1, 0.0, 0.0, 0.1]\n",
    "win =[1, 1, 0, 1]\n",
    "sad = [0.1, 0.0, 0.1, 0.2]\n",
    "\n",
    "alpha = 0.01\n",
    "input = [toes[0],wlrec[0],nfans[0]]\n",
    "true = [hurt[0], win[0], sad[0]]\n",
    "\n",
    "pred = neural_network(input, weights)\n",
    "\n",
    "error = [0,0,0]\n",
    "delta = [0,0,0]\n",
    "\n",
    "for i in range(len(true)):\n",
    "    error[i] = (pred[i] - true[i]) ** 2\n",
    "    delta[i] = pred[i] - true[i]\n",
    "\n",
    "def outer_prod(vec_a, vec_b):\n",
    "    out = np.zeros((len(vec_a), len(vec_b)))\n",
    "        \n",
    "    for i in range(len(vec_a)):\n",
    "        for j in range(len(vec_b)):\n",
    "            out[i][j] = vec_a[i]*vec_b[j]\n",
    "    return out\n",
    "\n",
    "#updating the weights\n",
    "weight_deltas = outer_prod(input,delta) * alpha\n",
    "weights = np.array(weights) - weight_deltas\n",
    "\n",
    "neural_network([toes[1],wlrec[1],nfans[1]], weights), [hurt[1], win[1], sad[1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the below is a shitfight\n",
    "The [digits dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) contains 1,797 8x8 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   5., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,  10.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,  16.,   9.,   0.],\n",
       "       ..., \n",
       "       [  0.,   0.,   1., ...,   6.,   0.,   0.],\n",
       "       [  0.,   0.,   2., ...,  12.,   0.,   0.],\n",
       "       [  0.,   0.,  10., ...,  12.,   1.,   0.]])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "print(digits.data.shape)\n",
    "digits.data # the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a visual representation of the image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11277d908>"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH0iI4gMJUXwgof8A4C6Y4wlBav8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112927c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(digits.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "digits.target contains what each datapoint represents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(digits.target.shape)\n",
    "digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the image above, which looks like a zero, so digits.data[0] should be equal to digits.target[0], is equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now to build out a nerual net to classify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = digits.data\n",
    "targets = digits.target\n",
    "#one weight per input pixel\n",
    "hidden_weights = np.random.random([64,10])\n",
    "output_weights = np.random.random(10)\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dot product of each input vector and the weights gives us 10 outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_input = np.dot(inputs[0], hidden_weights)\n",
    "hidden_output = sigmoid(hidden_input)\n",
    "hidden_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the dot product of the hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (64,) and (10,) not aligned: 64 (dim 0) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-471-4176378f2761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: shapes (64,) and (10,) not aligned: 64 (dim 0) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(a, output_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the output numbers seem pretty high, so we might want to normalize the incoming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#inputs = np.random.random([len(digits.data),64]) # a blank input\n",
    "#for i, item in enumerate(digits.data):\n",
    "#    inputs[i] = ((item - digits.data.mean()) / digits.data.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.8117562 , -0.8117562 ,  0.01925204, ..., -0.8117562 ,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [-0.8117562 , -0.8117562 , -0.8117562 , ...,  0.85026027,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [-0.8117562 , -0.8117562 , -0.8117562 , ...,  1.84747015,\n",
       "         0.68405863, -0.8117562 ],\n",
       "       ..., \n",
       "       [-0.8117562 , -0.8117562 , -0.64555455, ...,  0.18545368,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [-0.8117562 , -0.8117562 , -0.4793529 , ...,  1.18266357,\n",
       "        -0.8117562 , -0.8117562 ],\n",
       "       [-0.8117562 , -0.8117562 ,  0.85026027, ...,  1.18266357,\n",
       "        -0.64555455, -0.8117562 ]])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculating the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -3.06830336,  -4.76420192,  -4.85003619, ...,  -2.76972724,\n",
       "          2.21539197,   0.93605365],\n",
       "       [ -3.52793678,   0.08871104,  -3.77219696, ...,  -3.08459096,\n",
       "          2.10678359,   0.32016976],\n",
       "       [  1.51288988,   2.80620158,  -2.84952504, ...,  -0.77463449,\n",
       "          6.00694037,   4.89467518],\n",
       "       ..., \n",
       "       [  0.80117623,   3.1278505 ,  -1.25573866, ...,   3.34908273,\n",
       "          8.75502707,   7.78887479],\n",
       "       [ -1.41450206,   0.37686184,  -0.51405872, ...,   0.62209819,\n",
       "          6.90003339,   4.66896045],\n",
       "       [  2.31815972,   5.54513861,   1.42283582, ...,   4.83149732,\n",
       "         10.66877677,   9.76844909]])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_inputs = np.dot(inputs, hidden_weights)\n",
    "print(hidden_inputs.shape)\n",
    "hidden_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sigmoid function to calculate the output of the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.04443381,  0.00845755,  0.00776729, ...,  0.05898215,\n",
       "         0.90162322,  0.71830182],\n",
       "       [ 0.02852771,  0.52216323,  0.0224843 , ...,  0.04374736,\n",
       "         0.89156076,  0.57936562],\n",
       "       [ 0.81948909,  0.94301003,  0.05470587, ...,  0.31547743,\n",
       "         0.99754444,  0.99256929],\n",
       "       ..., \n",
       "       [ 0.69022603,  0.95802704,  0.22170833, ...,  0.96607479,\n",
       "         0.99984236,  0.99958585],\n",
       "       [ 0.19552493,  0.593116  ,  0.37424255, ...,  0.6506956 ,\n",
       "         0.99899326,  0.99070519],\n",
       "       [ 0.91036989,  0.9961088 ,  0.8057826 , ...,  0.99208852,\n",
       "         0.99997674,  0.99994277]])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_outputs = sigmoid(hidden_inputs)\n",
    "print(hidden_outputs.shape)\n",
    "hidden_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer gives 10 outputs for each of the 1797 data points.\n",
    "\n",
    "Now to predict the output neural layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3., ...,  4.,  3.,  5.])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_pred = np.dot(hidden_outputs, output_weights) \n",
    "final_outputs = output_pred.round(0)\n",
    "print(final_outputs.shape)\n",
    "final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the output errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1.,   1.,   1., ...,  16.,  36.,   9.])"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_errors = (final_outputs - digits.target)**2\n",
    "print(output_errors.shape)\n",
    "output_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., -3., -4., -2., -4., -6., -4., -7.])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_delta = final_outputs - digits.target\n",
    "output_delta[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how to adust the weights now that we have the output errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to multiply each of the 1797 final errors with the output weights. So we need to iterate through each of the data arrays and corresponding targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#weights\n",
    "ih_weight = np.random.random([64,10])\n",
    "ho_weight = np.random.random(10)\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for input, target in zip(digits.data, digits.target):\n",
    "        hidden_layer = np.dot(input, ih_weight)\n",
    "        hidden_outputs = sigmoid(hidden_inputs)\n",
    "\n",
    "        output_pred = np.dot(hidden_outputs, ho_weight) \n",
    "        output = output_pred.round(0)\n",
    "\n",
    "        # so now we have a prediction. time to back propagate!\n",
    "\n",
    "        #starting with the output errors:\n",
    "        error = output - target\n",
    "\n",
    "        output_error = error * output * (1 - output)\n",
    "\n",
    "        output_weight_delta = np.dot(ho_weight, output_delta) * learning_rate\n",
    "\n",
    "        ho_weight -= output_weight_delta * learning_rate\n",
    "    \n",
    "def neural_net(input)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -528.315657  ,  -498.88232425,  -965.34318155, -1118.71371346,\n",
       "        -913.29027258,  -461.29240453,  -609.00620295, -1384.59160212,\n",
       "        -428.01193233,  -584.50716073])"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_weight_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 17615.80505652,  16634.39963162,  32187.75948365,  37301.64425246,\n",
       "        30452.14198882,  15381.02640839,  20306.2968269 ,  46166.89932013,\n",
       "        14271.3445303 ,  19489.41709594])"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ih_weight_delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
