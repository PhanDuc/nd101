{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called hello_constant\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow stores data as tensors, which is an onject which can store stuff.\n",
    "\n",
    "constant data is stored as a [tf.constant](http://devdocs.io/tensorflow~python/tf/constant). The output of a constant tensor doesn't change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A is a 0-dimensional int32 tensor\n",
    "A = tf.constant(1234) \n",
    "# B is a 1-dimensional int32 tensor\n",
    "B = tf.constant([123,456,789]) \n",
    " # C is a 2-dimensional int32 tensor\n",
    "C = tf.constant([ [123,456,789], [222,333,444] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensorflow session is an environment for running a graph.\n",
    "\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)```\n",
    "    \n",
    "The `sess.run` function runs the hello_constant tensor created above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow can't just take a dataset x as in input, it has to be a tensor. [tf.placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) is used for dynamic data, which is fed into a tensor part at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test String\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_5:0\", shape=(), dtype=int32) Tensor(\"Sub_7:0\", shape=(), dtype=int32) Tensor(\"Mul_2:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.add(5, 2)  # 7\n",
    "y = tf.subtract(10, 4) # 6\n",
    "z = tf.multiply(2, 5)  # 10\n",
    "print(x ,y ,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "math operations have to use tensors of the same type, so watch out for floats, ints, etc. The below fails becuase that tries to subtract an int from a float:\n",
    "```python\n",
    "tf.subtract(tf.constant(2.0),tf.constant(1))```\n",
    "\n",
    "Use [tf.cast](http://devdocs.io/tensorflow~python/tf/cast) to convert a tensor to another type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sub_5:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to print the value of a tensorflow variable, you have to run it in a session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_13:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.add(5, 2)  # 7\n",
    "with tf.Session() as s:\n",
    "    output = s.run(x)\n",
    "print(x) # prints out a tensor object\n",
    "output # prints the output of the tensor x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Convert the following to TensorFlow:\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "x_minus_y = tf.divide(x,y)\n",
    "z = tf.subtract(x_minus_y, tf.cast(1.0, tf.float64))\n",
    "\n",
    "# TODO: Print z from a session\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "this is the central building block of ML.\n",
    "\n",
    "Logistic classifiers use a linear function like Wx + b = y and a softmax funtion to assign probablities to output.\n",
    "\n",
    "![](images/logistic_classifier.png)\n",
    "\n",
    "Softmax can take any kind of scores and turn them into probalities.\n",
    "\n",
    "> The most common operation in neural networks is calculating the linear combination of inputs, weights, and biases.\n",
    "\n",
    "> Here, W is a matrix of the weights connecting two layers. The output y, the input x, and the biases b are all vectors.\n",
    "\n",
    "Both weights and biases need to be modified as the NN trains, so they use a [tf.variable](https://www.tensorflow.org/api_docs/python/tf/Variable) class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable) class stores its state in the session, so it must be initialized manually. the [tf.global_variables_initializer()](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer) function initializes all variable tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an initial value for weights and biases, and normally we just use random values from a normal distrubution, for which we can use the [tf.truncated_normal()](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truncated_normal_9:0' shape=(120, 5) dtype=float32>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "weights.initial_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are already randomized, so there isn't really a need to randominize the bias, so we can use the [tf.zeros](https://www.tensorflow.org/api_docs/python/tf/zeros) function to generate a tensor with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zeros_2:0' shape=(5,) dtype=float32>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "bias.initial_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Softmax\n",
    "\n",
    "[softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) returns an array of prob values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65900117,  0.24243298,  0.09856589], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how i used softmax - fed softmax the logit_data directly\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # TODO: Calculate the softmax of the logits\n",
    "    softmax = tf.nn.softmax([2.0, 1.0, 0.1])     \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed in the logit data\n",
    "        output = sess.run(softmax)\n",
    "\n",
    "    return output\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65900117,  0.24243298,  0.09856589], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# udacity's version - they made a feed_dict\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "\n",
    "Use [LabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) to turn labels into one hot encoded vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Example labels\n",
    "labels = np.array([1,5,3,2,1,4,2,1,3])\n",
    "\n",
    "# Create the encoder\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Here the encoder finds the classes and assigns one-hot vectors \n",
    "lb.fit(labels)\n",
    "\n",
    "# And finally, transform the labels into one-hot encoded vectors\n",
    "lb.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy in Tensorflow\n",
    "\n",
    "We use [tf.reduce_sum](https://www.tensorflow.org/api_docs/python/tf/reduce_sum) and [tf.log](https://www.tensorflow.org/api_docs/python/tf/log) to calculate cross-entropy in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.reduce_sum([1, 2, 3, 4, 5])  # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.log(100.0)  # 4.60517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Print cross entropy from session\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data\n",
    "\n",
    "![](images/normalize_data.png)\n",
    "\n",
    "Data should have zero mean and equal variance, else it can mess up all the maths. two main points:\n",
    "- datasets can have some big and some vaules, like say house value is large (600,000), swimming pools is small (0 or 1), yet both data points are important in evaluating a house. So normalizing the data gives all the data a chance to make a difference in the output.\n",
    "- math errors can happen when mixing large and small numbers, as the example below demonstrates.\n",
    "\n",
    "In the example below, using a bg vs small initial number leads to a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.95367431640625, 0.9999999999177334)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = 1000000000, 1\n",
    "for i in range(1000000):\n",
    "    a = a + 1e-6\n",
    "    b = b + 1e-6\n",
    "a - 1000000000, b-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to initialize weights but a good rule of thumb is to start as a random distrubution around zero with a small standard deviation. A small sigma implies uncertainity, while large sigma implies certainity, so it's good to start with a small standard dev."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
