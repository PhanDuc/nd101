{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building a simple neural network\n",
    "\n",
    "A net takes in inputs, them multiplies each input by a corresponding weight and sums them together and adds this to a constant. This is then passed through a function to spit out an output.\n",
    "\n",
    "Here, we're mostly using the sigmoid function.\n",
    "\n",
    "## the sigmoid function\n",
    "\n",
    "<img src=\"images/sigmoid.png\" width=500 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD3CAYAAAAALt/WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111d73780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(-10,10)]\n",
    "#print(x)\n",
    "\n",
    "def sigmoid(num):\n",
    "    return 1.0 / (1.0 + np.exp(-num))\n",
    "\n",
    "plt.plot(range(-10,10), [sigmoid(i) for i in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "A simple way to think of this: If you are standing on top of a mountain of errors, then one way to get to the solution is to take a tiny step towards the correct direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP//Z\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/29PmNG7fuuM\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x111d3b358>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('29PmNG7fuuM', width=\"560\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "x = np.array([0.1, 0.3])\n",
    "y = 0.2\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "# The neural network output\n",
    "nn_output = sigmoid(x[0]*weights[0] + x[1]*weights[1])\n",
    "# or nn_output = sigmoid(np.dot(x, w))\n",
    "\n",
    "# output error\n",
    "error = y - nn_output\n",
    "\n",
    "# error gradient\n",
    "error_grad = error * sigmoid_prime(np.dot(x,weights))\n",
    "\n",
    "# Gradient descent step\n",
    "del_w = [ learnrate * error_grad * x[0],\n",
    "          learnrate * error_grad * x[1]]\n",
    "# or del_w = learnrate * error_grad * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple Gradient descent exercise\n",
    "\n",
    "Below, you'll calculate one gradient descent step for the weights of a simple network with two inputs and one output unit with a sigmoid activation function.\n",
    "\n",
    "Your goal here is to calculate the correct weight step using gradient descent. Remember that the weight step is the learning rate times the error times the input values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.377540668798\n",
      "Amount of Error:\n",
      "0.122459331202\n",
      "Change in Weights:\n",
      "[ 0.0143892  0.0287784]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5])\n",
    "\n",
    "# Calculate one gradient descent step for each weight\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(np.dot(x,w)) \n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error * nn_output * (1 - nn_output) * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing gradient descent \n",
    "\n",
    "use gradient descent to train a network on graduate school admissions data (found at http://www.ats.ucla.edu/stat/data/binary.csv. This dataset has three input features: GRE score, GPA, and the rank of the undergraduate school (numbered 1 through 4). Institutions with rank 1 have the highest prestige, those with rank 4 have the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  rank\n",
       "0      0  380  3.61     3\n",
       "1      1  660  3.67     3\n",
       "2      1  800  4.00     1\n",
       "3      1  640  3.19     4\n",
       "4      0  520  2.93     4"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions = pd.read_csv('data/binary.csv')\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  rank_1  rank_2  rank_3  rank_4\n",
       "0      0  380  3.61       0       0       1       0\n",
       "1      1  660  3.67       0       0       1       0\n",
       "2      1  800  4.00       1       0       0       0\n",
       "3      1  640  3.19       0       0       0       1\n",
       "4      0  520  2.93       0       0       0       1"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.798011</td>\n",
       "      <td>0.578348</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.625884</td>\n",
       "      <td>0.736008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.837832</td>\n",
       "      <td>1.603135</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.452749</td>\n",
       "      <td>-0.525269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.586063</td>\n",
       "      <td>-1.208461</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit       gre       gpa  rank_1  rank_2  rank_3  rank_4\n",
       "0      0 -1.798011  0.578348       0       0       1       0\n",
       "1      1  0.625884  0.736008       0       0       1       0\n",
       "2      1  1.837832  1.603135       1       0       0       0\n",
       "3      1  0.452749 -0.525269       0       0       0       1\n",
       "4      0 -0.586063 -1.208461       0       0       0       1"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((360, 7), (40, 7))"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the actual implmentation\n",
    "\n",
    "the dataset:\n",
    "features, targets, features_test, targets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.2571513862491301\n",
      "Train loss:  0.197830424060288\n",
      "Train loss:  0.19783042822776664   WARNING - Loss Increasing\n",
      "Train loss:  0.19783042822799557   WARNING - Loss Increasing\n",
      "Train loss:  0.1978304282279958   WARNING - Loss Increasing\n",
      "Train loss:  0.1978304282279958\n",
      "Train loss:  0.1978304282279958\n",
      "Train loss:  0.1978304282279958\n",
      "Train loss:  0.1978304282279958\n",
      "Train loss:  0.1978304282279958\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 10000\n",
    "learnrate = -0.015\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(np.dot(x,weights))\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = output - y\n",
    "\n",
    "        # TODO: Calculate change in weights\n",
    "        del_w += error * output * (1 - output) * x\n",
    "\n",
    "        # TODO: Update weights\n",
    "        weights += (del_w * learnrate) / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptrons\n",
    "\n",
    "for a single neuron, you just have one set of weights, but for multiple layers, you need a set of weights per layer.\n",
    "so you go from a an array to a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-21.66143804,  66.68201464],\n",
       "       [ -0.48590009, -38.07759344],\n",
       "       [ 29.61161684, -43.9503714 ],\n",
       "       [  7.51908942, -70.54812446],\n",
       "       [-47.81469776,   7.08700449],\n",
       "       [ 26.58479688,   6.16925812]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "weights = np.random.normal(0, 1/n_inputs**-2, size=(n_inputs, n_hidden))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[ 0.41492192  0.42604313  0.5002434 ]\n",
      "Output-layer Output:\n",
      "[ 0.49815196  0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_in_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_out = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_in_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_out)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propogate\n",
    "\n",
    "Similar to how inputs are propageted through the network, back propogate is sending the errors backwards through the network.\n",
    "\n",
    "A short overview:\n",
    "\n",
    "- [Udacity's 1min overview](https://www.youtube.com/watch?v=MZL97-2joxQ)\n",
    "- [series of short videos on neural networks](https://www.youtube.com/playlist?list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU)\n",
    "- [long lecture, useful](https://www.youtube.com/watch?v=i94OvYb6noo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[  1.77005547e-04  -5.11178506e-04]\n",
      " [  3.54011093e-05  -1.02235701e-04]\n",
      " [ -7.08022187e-05   2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error gradient for output layer\n",
    "del_err_output = error * output * (1 - output)\n",
    "\n",
    "# TODO: Calculate error gradient for hidden layer\n",
    "del_err_hidden = np.dot(del_err_output, weights_hidden_output) * \\\n",
    "                 hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * del_err_output * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_o = learnrate * del_err_hidden * x[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement backprop.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.22938805871942275\n",
      "Train loss:  0.22919565585046983\n",
      "Train loss:  0.22900788563931743\n",
      "Train loss:  0.22882511525327295\n",
      "Train loss:  0.22864772197908653\n",
      "Train loss:  0.22847609367853955\n",
      "Train loss:  0.22831062923349468\n",
      "Train loss:  0.22815173894979593\n",
      "Train loss:  0.22799984488533198\n",
      "Train loss:  0.2278553810641171\n",
      "Prediction accuracy: 0.750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 3  # number of hidden units\n",
    "epochs = 500\n",
    "learnrate = -0.02\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_activations = sigmoid(hidden_input)\n",
    "        output = np.dot(hidden_activations, weights_hidden_output)\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate error gradient in output unit\n",
    "        output_error = error * output * (1 - output)\n",
    "\n",
    "        # TODO: propagate errors to hidden layer\n",
    "        hidden_error = np.dot(output_error, weights_hidden_output) \\\n",
    "        * hidden_activations * (1 - hidden_activations)\n",
    "\n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += hidden_error * hidden_activations\n",
    "        del_w_input_hidden += hidden_error * x[:, None]\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_activations = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_activations,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
